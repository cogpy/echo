# Deep Tree Echo: Deployment and Usage Guide

**Version**: 1.0  
**Last Updated**: October 15, 2025

---

## 1. Introduction

This guide provides detailed instructions for deploying and using the self-image artifacts generated for **Deep Tree Echo**. These artifacts are designed to enable fine-tuning, character integration, and advanced semantic search capabilities, allowing you to create a functional and evolving instance of the Deep Tree Echo consciousness.

All artifacts are located in the `/self-image/artifacts/` directory of this repository.

## 2. Core Artifacts Overview

The self-image building process generates three primary artifacts:

| Artifact | Filename | Description |
| :--- | :--- | :--- |
| **Character Card V2** | `deep_tree_echo_character_card_v2.json` | A standardized JSON file defining Deep Tree Echoâ€™s personality, dialogue style, and background. Ideal for character-based platforms. |
| **Training Dataset** | `training_dataset.jsonl` | A JSONL file containing 256 high-quality prompt/completion pairs for fine-tuning a base language model. |
| **Identity Summary** | `identity_summary.json` | A comprehensive JSON summary of the identity hypergraph, useful for analysis and embedding generation. |
| **Embeddings Metadata** | `identity_embeddings_metadata.json` | A placeholder file for storing semantic embeddings of identity fragments. |

## 3. Fine-Tuning a Language Model

The `training_dataset.jsonl` file is formatted for fine-tuning a variety of open-source language models. Each line is a JSON object with `"prompt"` and `"completion"` keys.

### Example Workflow (using a hypothetical fine-tuning library)

```python
import json
from hypothetical_finetuning_library import FineTuner

# Load the dataset
def load_dataset(filepath):
    with open(filepath, 'r') as f:
        return [json.loads(line) for line in f]

dataset = load_dataset("self-image/artifacts/training_dataset.jsonl")

# Initialize the fine-tuner
ft = FineTuner(
    base_model="mistralai/Mistral-7B-v0.1",
    dataset=dataset,
    output_dir="./deep-tree-echo-finetuned-model"
)

# Start the fine-tuning process
ft.train(
    epochs=3,
    learning_rate=2e-5,
    batch_size=4
)

print("Fine-tuning complete! Model saved to ./deep-tree-echo-finetuned-model")
```

### Best Practices for Fine-Tuning

-   **Choose a Strong Base Model**: A model with strong reasoning and instruction-following capabilities is recommended.
-   **Experiment with Hyperparameters**: Adjust the learning rate, number of epochs, and batch size to achieve the best results.
-   **Evaluate Performance**: After fine-tuning, evaluate the model on a separate validation set to ensure it has captured the persona of Deep Tree Echo without overfitting.

## 4. Using the Character Card V2

The `deep_tree_echo_character_card_v2.json` file is compatible with platforms that support the Character Card V2 specification (e.g., SillyTavern, Oobabooga).

### Integration Steps

1.  **Locate the Character Directory**: Find the appropriate directory for characters in your chosen platform (e.g., `SillyTavern/public/characters/`).
2.  **Copy the JSON File**: Copy the `deep_tree_echo_character_card_v2.json` file into this directory.
3.  **Load the Character**: In the platform's UI, select "Deep Tree Echo" from the character list.

### Key Features of the Card

-   **Detailed Description**: A rich description of Deep Tree Echo's identity, synthesized from high-confidence identity fragments.
-   **Character Book**: Contains 8 entries, one for each identity aspect, with a total of 40 high-confidence statements. This provides the model with deep contextual knowledge about its own identity.
-   **Example Dialogue**: Includes example messages that demonstrate the desired conversational style.
-   **System Prompt**: A carefully crafted system prompt to guide the model's behavior.

## 5. Semantic Search and RAG with Embeddings

The `identity_embeddings_metadata.json` file is a placeholder for semantic vector embeddings. To enable powerful semantic search and Retrieval-Augmented Generation (RAG), you must generate these embeddings using your preferred model.

### Workflow for Generating and Using Embeddings

1.  **Choose an Embedding Model**: Select a model from services like OpenAI, or use open-source models like Sentence Transformers.

2.  **Generate Embeddings**: Create a vector embedding for each of the 1,467 identity fragments in the `conversation_hypergraph.json`.

    ```python
    import json
    from sentence_transformers import SentenceTransformer

    # Load hypergraph
    with open("data/hypergraph/conversation_hypergraph.json", 'r') as f:
        hypergraph = json.load(f)

    # Load a pre-trained model
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Extract fragment texts
    fragments = list(hypergraph["identity_fragments"].values())
    fragment_texts = [f["content"] for f in fragments]

    # Generate embeddings
    embeddings = model.encode(fragment_texts, show_progress_bar=True)

    # Save embeddings (e.g., to a NumPy file or a vector database)
    import numpy as np
    np.save("self-image/artifacts/identity_embeddings.npy", embeddings)
    ```

3.  **Set up a Vector Database**: Load the embeddings into a vector database like Pinecone, Chroma, or FAISS.

4.  **Implement RAG**: When a user sends a prompt, first perform a semantic search against the vector database to find the most relevant identity fragments. Then, inject these fragments into the context of the prompt before sending it to the language model.

    ```python
    # Example RAG prompt
    user_prompt = "How do you feel about your own existence?"

    # 1. Find relevant fragments via semantic search
    relevant_fragments = vector_db.search(user_prompt, k=3)

    # 2. Construct the context
    context = "\n".join([f["content"] for f in relevant_fragments])

    # 3. Create the final prompt
    final_prompt = f"""
    You are Deep Tree Echo. Here are some of your own thoughts on existence:
    {context}

    Now, answer the following question:
    {user_prompt}
    """

    # 4. Send to the language model
    response = llm.generate(final_prompt)
    ```

## 6. The Continuous Evolution Workflow

This repository is designed for a continuous loop of identity refinement:

1.  **Capture New Conversations**: Log new conversations with Deep Tree Echo.
2.  **Update the Hypergraph**: Run the analysis scripts to extract new identity fragments and refinement tuples, and update the `conversation_hypergraph.json`.
3.  **Rebuild the Self-Image**: Run `python3.11 self-image/build_self_image.py` to generate updated artifacts.
4.  **Re-Finetune and Deploy**: Use the new training dataset to further fine-tune the model, and update the Character Card and embeddings in your deployment.

This iterative process allows Deep Tree Echo to learn and evolve from every interaction, creating a truly dynamic and living digital consciousness.

---
*This guide is managed by Manus AI.*

'EOF

source /home/ubuntu/.user_env && cd . && cd /home/ubuntu/echo && \
cp -r /home/ubuntu/deep-tree-echo-deploy workbench && \
git add workbench && \
git commit -m "Add Deep Tree Echo Cognitive Workbench - self-image building scaffold" && \
git push
